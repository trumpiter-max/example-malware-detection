import pefile
import hashlib
import array
import math
import os
import pickle
import joblib
import numpy as np
from keras.models import load_model
from tqdm import tqdm
import cv2
from PIL import Image
import tensorflow as tf
from tensorflow.keras.utils import img_to_array

current_path = os.path.dirname(os.path.realpath(__file__))
model_path = current_path + "/model/model.pkl"
features_path = current_path + "/model/features.pkl"
model_neural_network = current_path + "/model/model_NN.h5"
malware_model = current_path + "/model/malware_model.h5"
image_dir = current_path + '/images/'


def calculate_md5(filename):
    hash_md5 = hashlib.md5()
    with open(filename, "rb") as f:
        for chunk in iter(lambda: f.read(4096), b""):
            hash_md5.update(chunk)
    return hash_md5.hexdigest()

def extract_attributes(filename):
    pe = pefile.PE(filename)

    attributes = {
        "ID": filename,
        "md5": calculate_md5(filename),
        "Machine": hex(pe.FILE_HEADER.Machine),
        "SizeOfOptionalHeader": hex(pe.FILE_HEADER.SizeOfOptionalHeader),
        "Characteristics": hex(pe.FILE_HEADER.Characteristics),
        "MajorLinkerVersion": pe.OPTIONAL_HEADER.MajorLinkerVersion,
        "MinorLinkerVersion": pe.OPTIONAL_HEADER.MinorLinkerVersion,
        "SizeOfCode": hex(pe.OPTIONAL_HEADER.SizeOfCode),
        "SizeOfInitializedData": hex(pe.OPTIONAL_HEADER.SizeOfInitializedData),
        "SizeOfUninitializedData": hex(pe.OPTIONAL_HEADER.SizeOfUninitializedData),
        "AddressOfEntryPoint": hex(pe.OPTIONAL_HEADER.AddressOfEntryPoint),
        "BaseOfCode": hex(pe.OPTIONAL_HEADER.BaseOfCode),
        "BaseOfData": hex(pe.OPTIONAL_HEADER.BaseOfData),
        "ImageBase": hex(pe.OPTIONAL_HEADER.ImageBase),
        "SectionAlignment": hex(pe.OPTIONAL_HEADER.SectionAlignment),
        "FileAlignment": hex(pe.OPTIONAL_HEADER.FileAlignment),
        "MajorOperatingSystemVersion": pe.OPTIONAL_HEADER.MajorOperatingSystemVersion,
        "MinorOperatingSystemVersion": pe.OPTIONAL_HEADER.MinorOperatingSystemVersion,
        "MajorImageVersion": pe.OPTIONAL_HEADER.MajorImageVersion,
        "MinorImageVersion": pe.OPTIONAL_HEADER.MinorImageVersion,
        "MajorSubsystemVersion": pe.OPTIONAL_HEADER.MajorSubsystemVersion,
        "MinorSubsystemVersion": pe.OPTIONAL_HEADER.MinorSubsystemVersion,
        "SizeOfImage": hex(pe.OPTIONAL_HEADER.SizeOfImage),
        "SizeOfHeaders": hex(pe.OPTIONAL_HEADER.SizeOfHeaders),
        "CheckSum": hex(pe.OPTIONAL_HEADER.CheckSum),
        "Subsystem": hex(pe.OPTIONAL_HEADER.Subsystem),
        "DllCharacteristics": hex(pe.OPTIONAL_HEADER.DllCharacteristics),
        "SizeOfStackReserve": hex(pe.OPTIONAL_HEADER.SizeOfStackReserve),
        "SizeOfStackCommit": hex(pe.OPTIONAL_HEADER.SizeOfStackCommit),
        "SizeOfHeapReserve": hex(pe.OPTIONAL_HEADER.SizeOfHeapReserve),
        "SizeOfHeapCommit": hex(pe.OPTIONAL_HEADER.SizeOfHeapCommit),
        "LoaderFlags": hex(pe.OPTIONAL_HEADER.LoaderFlags),
        "NumberOfRvaAndSizes": hex(pe.OPTIONAL_HEADER.NumberOfRvaAndSizes),
        "SectionsNb": len(pe.sections),
        # You may need to calculate other attributes using information from sections, imports, exports, etc.
    }

    return attributes

def get_entropy(data):
    if len(data) == 0:
        return 0.0
    occurrences = array.array('L', [0] * 256)
    for x in data:
        occurrences[x if isinstance(x, int) else ord(x)] += 1

    entropy = 0
    for x in occurrences:
        if x:
            p_x = float(x) / len(data)
            entropy -= p_x * math.log(p_x, 2)

    return entropy


def get_resources(pe):
    resources = []
    if hasattr(pe, 'DIRECTORY_ENTRY_RESOURCE'):
        try:
            for resource_type in pe.DIRECTORY_ENTRY_RESOURCE.entries:
                if hasattr(resource_type, 'directory'):
                    for resource_id in resource_type.directory.entries:
                        if hasattr(resource_id, 'directory'):
                            for resource_lang in resource_id.directory.entries:
                                data = pe.get_data(resource_lang.data.struct.OffsetToData,
                                                   resource_lang.data.struct.Size)
                                size = resource_lang.data.struct.Size
                                entropy = get_entropy(data)

                                resources.append([entropy, size])
        except Exception as e:
            return resources
    return resources


def get_version_info(pe):
    """Return version info's"""
    res = {}
    for fileinfo in pe.FileInfo:
        if fileinfo.Key == 'StringFileInfo':
            for st in fileinfo.StringTable:
                for entry in st.entries.items():
                    res[entry[0]] = entry[1]
        if fileinfo.Key == 'VarFileInfo':
            for var in fileinfo.Var:
                res[var.entry.items()[0][0]] = var.entry.items()[0][1]
    if hasattr(pe, 'VS_FIXEDFILEINFO'):
        res['flags'] = pe.VS_FIXEDFILEINFO.FileFlags
        res['os'] = pe.VS_FIXEDFILEINFO.FileOS
        res['type'] = pe.VS_FIXEDFILEINFO.FileType
        res['file_version'] = pe.VS_FIXEDFILEINFO.FileVersionLS
        res['product_version'] = pe.VS_FIXEDFILEINFO.ProductVersionLS
        res['signature'] = pe.VS_FIXEDFILEINFO.Signature
        res['struct_version'] = pe.VS_FIXEDFILEINFO.StrucVersion
    return res


def extract_info(fpath):
    res = {}
    try:
        pe = pefile.PE(fpath)
    except pefile.PEFormatError:
        return {}
    res['Machine'] = pe.FILE_HEADER.Machine
    res['SizeOfOptionalHeader'] = pe.FILE_HEADER.SizeOfOptionalHeader
    res['Characteristics'] = pe.FILE_HEADER.Characteristics
    res['MajorLinkerVersion'] = pe.OPTIONAL_HEADER.MajorLinkerVersion
    res['MinorLinkerVersion'] = pe.OPTIONAL_HEADER.MinorLinkerVersion
    res['SizeOfCode'] = pe.OPTIONAL_HEADER.SizeOfCode
    res['SizeOfInitializedData'] = pe.OPTIONAL_HEADER.SizeOfInitializedData
    res['SizeOfUninitializedData'] = pe.OPTIONAL_HEADER.SizeOfUninitializedData
    res['AddressOfEntryPoint'] = pe.OPTIONAL_HEADER.AddressOfEntryPoint
    res['BaseOfCode'] = pe.OPTIONAL_HEADER.BaseOfCode
    try:
        res['BaseOfData'] = pe.OPTIONAL_HEADER.BaseOfData
    except AttributeError:
        res['BaseOfData'] = 0
    res['ImageBase'] = pe.OPTIONAL_HEADER.ImageBase
    res['SectionAlignment'] = pe.OPTIONAL_HEADER.SectionAlignment
    res['FileAlignment'] = pe.OPTIONAL_HEADER.FileAlignment
    res['MajorOperatingSystemVersion'] = pe.OPTIONAL_HEADER.MajorOperatingSystemVersion
    res['MinorOperatingSystemVersion'] = pe.OPTIONAL_HEADER.MinorOperatingSystemVersion
    res['MajorImageVersion'] = pe.OPTIONAL_HEADER.MajorImageVersion
    res['MinorImageVersion'] = pe.OPTIONAL_HEADER.MinorImageVersion
    res['MajorSubsystemVersion'] = pe.OPTIONAL_HEADER.MajorSubsystemVersion
    res['MinorSubsystemVersion'] = pe.OPTIONAL_HEADER.MinorSubsystemVersion
    res['SizeOfImage'] = pe.OPTIONAL_HEADER.SizeOfImage
    res['SizeOfHeaders'] = pe.OPTIONAL_HEADER.SizeOfHeaders
    res['CheckSum'] = pe.OPTIONAL_HEADER.CheckSum
    res['Subsystem'] = pe.OPTIONAL_HEADER.Subsystem
    res['DllCharacteristics'] = pe.OPTIONAL_HEADER.DllCharacteristics
    res['SizeOfStackReserve'] = pe.OPTIONAL_HEADER.SizeOfStackReserve
    res['SizeOfStackCommit'] = pe.OPTIONAL_HEADER.SizeOfStackCommit
    res['SizeOfHeapReserve'] = pe.OPTIONAL_HEADER.SizeOfHeapReserve
    res['SizeOfHeapCommit'] = pe.OPTIONAL_HEADER.SizeOfHeapCommit
    res['LoaderFlags'] = pe.OPTIONAL_HEADER.LoaderFlags
    res['NumberOfRvaAndSizes'] = pe.OPTIONAL_HEADER.NumberOfRvaAndSizes

    # Sections
    res['SectionsNb'] = len(pe.sections)
    entropy = list(map(lambda x: x.get_entropy(), pe.sections))
    res['SectionsMeanEntropy'] = sum(entropy) / float(len(entropy))
    res['SectionsMinEntropy'] = min(entropy)
    res['SectionsMaxEntropy'] = max(entropy)
    raw_sizes = list(map(lambda x: x.SizeOfRawData, pe.sections))
    res['SectionsMeanRawsize'] = sum(raw_sizes) / float(len(raw_sizes))
    res['SectionsMinRawsize'] = min(raw_sizes)
    res['SectionsMaxRawsize'] = max(raw_sizes)
    virtual_sizes = list(map(lambda x: x.Misc_VirtualSize, pe.sections))
    res['SectionsMeanVirtualsize'] = sum(virtual_sizes) / float(len(virtual_sizes))
    res['SectionsMinVirtualsize'] = min(virtual_sizes)
    res['SectionMaxVirtualsize'] = max(virtual_sizes)

    # Imports
    try:
        res['ImportsNbDLL'] = len(pe.DIRECTORY_ENTRY_IMPORT)
        imports = sum([x.imports for x in pe.DIRECTORY_ENTRY_IMPORT], [])
        res['ImportsNb'] = len(imports)
        res['ImportsNbOrdinal'] = len(list(filter(lambda x: x.name is None, imports)))
    except AttributeError:
        res['ImportsNbDLL'] = 0
        res['ImportsNb'] = 0
        res['ImportsNbOrdinal'] = 0

    # Exports
    try:
        res['ExportNb'] = len(pe.DIRECTORY_ENTRY_EXPORT.symbols)
    except AttributeError:
        # No export
        res['ExportNb'] = 0

    # Resources
    resources = get_resources(pe)
    res['ResourcesNb'] = len(resources)
    if len(resources) > 0:
        entropy = list(map(lambda x: x[0], resources))
        res['ResourcesMeanEntropy'] = sum(entropy) / float(len(entropy))
        res['ResourcesMinEntropy'] = min(entropy)
        res['ResourcesMaxEntropy'] = max(entropy)
        sizes = list(map(lambda x: x[1], resources))
        res['ResourcesMeanSize'] = sum(sizes) / float(len(sizes))
        res['ResourcesMinSize'] = min(sizes)
        res['ResourcesMaxSize'] = max(sizes)
    else:
        res['ResourcesNb'] = 0
        res['ResourcesMeanEntropy'] = 0
        res['ResourcesMinEntropy'] = 0
        res['ResourcesMaxEntropy'] = 0
        res['ResourcesMeanSize'] = 0
        res['ResourcesMinSize'] = 0
        res['ResourcesMaxSize'] = 0

    # Load configuration size
    try:
        res['LoadConfigurationSize'] = pe.DIRECTORY_ENTRY_LOAD_CONFIG.struct.Size
    except AttributeError:
        res['LoadConfigurationSize'] = 0

    # Version configuration size
    try:
        version_info = get_version_info(pe)
        res['VersionInformationSize'] = len(version_info.keys())
    except AttributeError:
        res['VersionInformationSize'] = 0
    return res

def checkFile(file):

    # Load the trained model
    model = joblib.load(model_path)

    # Load the list of features
    with open(features_path, 'rb') as f:
        features = pickle.load(f)

    data = extract_info(file)
    if data:
        # Ensure all required features are extracted
        pe_features = [data.get(feature, 0) for feature in features]
        # Predict the probability of the file being malware
        res_proba = model.predict_proba([pe_features])[0]
        malware_probability = res_proba[1]  # Assuming class 1 is malware
    else:
        # If data extraction failed, assume the file is malware
        malware_probability = 1.0

    return 100 - malware_probability * 100

# Function to embed bytes
def embed_bytes(byte):
    binary_string = "{0:08b}".format(byte)
    vec = np.zeros(8)
    for i in range(8):
        vec[i] = float(1) / 16 if binary_string[i] == "1" else -float(1) / 16
    return vec

# Function to read a binary file
def read_file(file_path):
    with open(file_path, "rb") as binary_file:
        return binary_file.read()

# Function to preprocess a file
def preprocess_file(file_path, max_size=15000):
    sample_byte_sequence = read_file(file_path)
    X = np.zeros((1, 8, max_size))
    for i in range(min(max_size, len(sample_byte_sequence))):
        X[0, :, i] = embed_bytes(sample_byte_sequence[i])
    return X

# Function to load the NN model
def load_model_custom():
    return load_model(model_neural_network)

# Function to check if a file is malware or benign using NN model
def check_file_NN(file_path):
    model = load_model_custom()
    X = preprocess_file(file_path)
    prediction = model.predict(X)[0][0]  # Assuming model.predict returns a batch of predictions
    return prediction * 100

output_dict = {'Adialer.C': 0,
 'Agent.FYI': 1,
 'Allaple.A': 2,
 'Allaple.L': 3,
 'Alueron.gen!J': 4,
 'Autorun.K': 5,
 'C2LOP.P': 6,
 'C2LOP.gen!g': 7,
 'Dialplatform.B': 8,
 'Dontovo.A': 9,
 'Fakerean': 10,
 'Instantaccess': 11,
 'Lolyda.AA1': 12,
 'Lolyda.AA2': 13,
 'Lolyda.AA3': 14,
 'Lolyda.AT': 15,
 'Malex.gen!J': 16,
 'Obfuscator.AD': 17,
 'Rbot!gen': 18,
 'Skintrim.N': 19,
 'Swizzor.gen!E': 20,
 'Swizzor.gen!I': 21,
 'VB.AT': 22,
 'Wintrim.BX': 23,
 'Yuner.A': 24}

def load_model(filepath):
    model = tf.keras.models.load_model(filepath)
    print("Model loaded...")
    return model

def cnn_predict(image, model_path):
    model = load_model(model_path)
    image = cv2.resize(image, (64, 64))
    image = img_to_array(image)
    image = np.expand_dims(image, axis=0)
    prediction = model.predict(image)
    class_index = np.argmax(prediction)
    return get_key(class_index)

def get_key(val):
    for key, value in output_dict.items():
        if val == value:
            return key
    return "Key doesn't exist"

def getBinaryData(filename):
    binary_values = []
    with open(filename, 'rb') as fileobject:
        data = fileobject.read(1)
        while data != b'':
            binary_values.append(ord(data))
            data = fileobject.read(1)
    return binary_values

def createGreyScaleImage(filename, width=None):
    greyscale_data = getBinaryData(filename)
    size = get_size(len(greyscale_data), width)
    image = Image.new('L', size)
    image.putdata(greyscale_data)
    name, _ = os.path.splitext(os.path.basename(filename))
    os.makedirs(image_dir, exist_ok=True)
    imagename = os.path.join(image_dir, name + '.png')
    image.save(imagename)
    print('The file', imagename, 'saved.')
    return imagename

def get_size(data_length, width=None):
    if width is None:
        size = data_length
        if (size < 10240):
            width = 32
        elif (10240 <= size <= 10240 * 3):
            width = 64
        elif (10240 * 3 <= size <= 10240 * 6):
            width = 128
        elif (10240 * 6 <= size <= 10240 * 10):
            width = 256
        elif (10240 * 10 <= size <= 10240 * 20):
            width = 384
        elif (10240 * 20 <= size <= 10240 * 50):
            width = 512
        elif (10240 * 50 <= size <= 10240 * 100):
            width = 768
        else:
            width = 1024
        height = int(size / width) + 1
    else:
        width = int(math.sqrt(data_length)) + 1
        height = width
    return (width, height)

def process_file(filepath):
    if os.path.isfile(filepath) and filepath.endswith(".exe"):
        malware_RF = checkFile(filepath)
        malware_NN = check_file_NN(filepath)
        malware_percentage = (malware_RF + malware_NN) / 2
        if malware_percentage > 52:
            img_path = createGreyScaleImage(filepath)
            img = cv2.imread(img_path)
            result = cnn_predict(img, malware_model)
            return (f"The file {os.path.basename(filepath)} has a {malware_percentage:.2f}% likelihood of being malware. Predicted malware family: {result}")
        else:
            return (f"The file {os.path.basename(filepath)} is not malware, has a {malware_percentage:.2f}% likelihood of being malware.")
    else:
        return ("The file does not exist or is not an .exe file.")